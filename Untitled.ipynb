{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All source files found\n",
      "\n",
      "Output directory /results2/MIDOSS/forcing/SalishSeaCast/hdf5/01feb19-07feb19/ created\n",
      "\n",
      "Time elapsed: 0:6:6\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2013-2016 The Salish Sea MEOPAR contributors\n",
    "# and The University of British Columbia\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Creates forcing HDF5 input files for MOHID based on user input time range\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import errno\n",
    "import time\n",
    "import h5py\n",
    "from salishsea_tools import utilities\n",
    "from salishsea_tools import viz_tools\n",
    "import multiprocessing\n",
    "\n",
    "#from scipy.interpolate import griddata\n",
    "\n",
    "# NEMO input files directory\n",
    "nemoinput = '/results2/SalishSea/nowcast-green.201806/'\n",
    "\n",
    "# HRDPS input files directory\n",
    "hdinput = '/results/forcing/atmospheric/GEM2.5/operational/'\n",
    "\n",
    "# WW3 input files directory\n",
    "wwinput = '/opp/wwatch3/nowcast/'\n",
    "\n",
    "# Output filepath\n",
    "outpath = '/results2/MIDOSS/forcing/SalishSeaCast/'\n",
    "\n",
    "\n",
    "## Integer -> String\n",
    "## consumes time in seconds and outputs a string that gives the time in HH:MM:SS format\n",
    "def conv_time(time):\n",
    "    \"\"\"Give time in HH:MM:SS format.\n",
    "\n",
    "    :arg time: time elapsed in seconds\n",
    "    :type integer: :py:class:'int'\n",
    "\n",
    "    :returns: time elapsed in HH:MM:SS format\n",
    "    :rtype: :py:class:`str'\n",
    "    \"\"\"\n",
    "    hours = int(time/3600)\n",
    "    mins = int((time - (hours*3600))/60)\n",
    "    secs = int((time - (3600 * hours) - (mins *60)))\n",
    "    return '{}:{}:{}'.format(hours, mins, secs)\n",
    "\n",
    "\n",
    "def generate_currents_hdf5(timestart, timeend, path, outpath, compression_level = 1):\n",
    "    \"\"\"Renerate current forcing HDF5 input file MOHID.\n",
    "\n",
    "    :arg timestart: date from when to start concatenating\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg timeend: date at which to stop concatenating\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg path: path of input files\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg outpath: path for output files\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg compression_level: compression level for output file (Integer[1,9])\n",
    "    :type integer: :py:class:'int'\n",
    "\n",
    "    :returns: None\n",
    "    :rtype: :py:class:`NoneType'\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate list of dates from daterange given\n",
    "    daterange = [parse(t) for t in [timestart, timeend]]\n",
    "    U_files = []\n",
    "    V_files = []\n",
    "    T_files = []\n",
    "\n",
    "    # append all filename strings within daterange to lists\n",
    "    for day in range(np.diff(daterange)[0].days):\n",
    "        datestamp = daterange[0] + timedelta(days=day)\n",
    "        datestr1 = datestamp.strftime('%d%b%y').lower()\n",
    "        datestr2 = datestamp.strftime('%Y%m%d')\n",
    "        \n",
    "        # check if file exists. exit if it does not. add path to list if it does.\n",
    "            # U files\n",
    "        U_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_U.nc'\n",
    "        if not os.path.exists(U_path):\n",
    "            print(f'File {U_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        U_files.append(U_path)\n",
    "            # V files\n",
    "        V_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_V.nc'\n",
    "        if not os.path.exists(V_path):\n",
    "            print(f'File {V_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        V_files.append(V_path)\n",
    "            # T files\n",
    "        T_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_T.nc'\n",
    "        if not os.path.exists(T_path):\n",
    "            print(f'File {T_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        T_files.append(T_path)\n",
    "        \n",
    "    print('\\nAll source files found')\n",
    "    # string: output folder name with date ranges used. end date will be lower by a day than timeend because datasets only go until midnight\n",
    "    folder = str(datetime(parse(timestart).year, parse(timestart).month, parse(timestart).day).strftime('%d%b%y').lower()) + '-' + str(datetime(parse(timeend).year, parse(timeend).month, parse(timeend).day-1).strftime('%d%b%y').lower())\n",
    "    # create output directory\n",
    "    dirname = f'{outpath}hdf5/{folder}/'\n",
    "    if not os.path.exists(os.path.dirname(dirname)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(dirname))\n",
    "        except OSError as exc:\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    print(f'\\nOutput directory {dirname} created\\n')\n",
    "    # create hdf5 file and create tree structure\n",
    "    f = h5py.File(f'{dirname}foocurrents.hdf5', 'w')\n",
    "    times = f.create_group('Time')\n",
    "    velocity_u = f.create_group('/Results/velocity U')\n",
    "    velocity_v = f.create_group('/Results/velocity V')\n",
    "    water_level = f.create_group('/Results/water level')\n",
    "    return f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level\n",
    "    \n",
    "#for file_index in bar(range(number_of_files)):\n",
    "def write_currents (f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level, file_index):\n",
    "    attr_counter = file_index * len(U_files)\n",
    "    U_raw = xr.open_dataset(U_files[file_index])\n",
    "    V_raw = xr.open_dataset(V_files[file_index])\n",
    "    T_raw = xr.open_dataset(T_files[file_index])\n",
    "    # assume all files have same time_counter markers\n",
    "    datelist = U_raw.time_counter.values.astype('datetime64[s]').astype(datetime)\n",
    "    # unstagger to move U, V to center of grid square\n",
    "    U  = viz_tools.unstagger_xarray(U_raw.vozocrtx, 'x')\n",
    "    V  = viz_tools.unstagger_xarray(V_raw.vomecrty, 'y')\n",
    "    # convert xarrays to numpy arrays and cut off grid edges\n",
    "    U = U.values[...,:,1:897:,1:397]\n",
    "    V = V.values[...,:,1:897:,1:397]\n",
    "    sea_surface = T_raw.sossheig.values[...,:,1:897:,1:397]\n",
    "    # rotate currents to True North\n",
    "    current_u, current_v = viz_tools.rotate_vel(U, V)\n",
    "    # clear memory\n",
    "    U, V = None, None\n",
    "    # transpose grid (rotate 90 clockwise)\n",
    "    current_u = np.transpose(current_u, [0,1,3,2])\n",
    "    current_v = np.transpose(current_v, [0,1,3,2])\n",
    "    sea_surface = np.transpose(sea_surface, [0,2,1])\n",
    "    # flip currents by depth dimension\n",
    "    current_u = np.flip(current_u, axis = 1)\n",
    "    current_v = np.flip(current_v, axis = 1)\n",
    "    # convert nans to 0's and set datatype to float64\n",
    "    current_u = np.nan_to_num(current_u).astype('float64')\n",
    "    current_v = np.nan_to_num(current_v).astype('float64')\n",
    "    sea_surface = np.nan_to_num(sea_surface).astype('float64')\n",
    "    # make list of time arrays\n",
    "    datearrays = []\n",
    "    for date in datelist:\n",
    "        datearrays.append(np.array([date.year, date.month, date.day, date.hour, date.minute, date.second]).astype('float64'))\n",
    "    # write u wind values to hdf5\n",
    "    for i in range(current_u.shape[0]):\n",
    "        velocity_attr = 'velocity U_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = velocity_u.create_dataset(velocity_attr, shape = (40, 396, 896), data = current_u[i],chunks=(40, 396, 896), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'FillValue' : np.array([0.]), 'Maximum' : np.array([5.]), 'Minimum' : np.array([-5.]), 'Units' : b'm/s'}\n",
    "        dset.attrs.update(metadata)\n",
    "\n",
    "    # write v wind values to hdf5\n",
    "    for i in range(current_v.shape[0]):\n",
    "        velocity_attr = 'velocity V_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = velocity_v.create_dataset(velocity_attr, shape = (40, 396, 896), data = current_v[i],chunks=(40, 396, 896), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'FillValue' : np.array([0.]), 'Maximum' : np.array([5.]), 'Minimum' : np.array([-5.]), 'Units' : b'm/s'}\n",
    "        dset.attrs.update(metadata)\n",
    "\n",
    "    # write  water level values to hdf5\n",
    "\n",
    "    for i in range(sea_surface.shape[0]):\n",
    "        level_attr = 'water level_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = water_level.create_dataset(level_attr, shape = (396, 896), data = sea_surface[i],chunks=(396, 896), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'FillValue' : np.array([0.]), 'Maximum' : np.array([5.]), 'Minimum' : np.array([-5.]), 'Units' : b'm'}\n",
    "        dset.attrs.update(metadata)\n",
    "\n",
    "    # write time values to hdf5\n",
    "\n",
    "    for i in range(len(datearrays)):\n",
    "        time_attr = 'Time_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = times.create_dataset(time_attr, shape = (6,), data = datearrays[i],chunks=(6,), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'Maximum' : np.array([datearrays[i][0].astype('float64')]), 'Minimum' : np.array([-0.]), 'Units' : b'YYYY/MM/DD HH:MM:SS'} # !!!\n",
    "        dset.attrs.update(metadata)\n",
    "    \n",
    "def manage_queue(current, remaining, workers):\n",
    "    if len(current) != 0:\n",
    "        for task in current:\n",
    "            print(task.is_alive())\n",
    "            if task.is_alive() == False:\n",
    "                try:\n",
    "                    task.join()\n",
    "                    current.remove(task)\n",
    "                except RuntimeError as err:\n",
    "                    if 'cannot join current thread' in err.args[0]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise\n",
    "    if len(current) != workers and len(remaining) != 0:\n",
    "        remaining[0].start()\n",
    "        current.append(remaining[0])\n",
    "        remaining.remove(remaining[0])\n",
    "        manage_queue(current, remaining, workers)\n",
    "    if len(current) == 0  and  len(remaining) == 0:\n",
    "        return\n",
    "    else:\n",
    "        manage_queue(current, remaining, workers)\n",
    "\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    beganat = time.time()\n",
    "    timestart = '1 Feb 2019'\n",
    "    timeend = '8 Feb 2019'\n",
    "    f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level = generate_currents_hdf5(timestart, timeend, nemoinput, outpath, compression_level = 1)\n",
    "    processes= []\n",
    "    #multiprocessing.set_start_method('spawn')\n",
    "    for i in range(len(U_files)):\n",
    "        p = multiprocessing.Process(target = write_currents, args = (f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level, i))\n",
    "        processes.append(p)\n",
    "    manage_queue([], processes, 4)\n",
    "    f.close()\n",
    "    print('Time elapsed: {}'.format(conv_time(time.time()-beganat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All source files found\n",
      "\n",
      "Output directory /results2/MIDOSS/forcing/SalishSeaCast/hdf5/01feb19-07feb19/ created\n",
      "\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n'''\nTraceback (most recent call last):\n  File \"/home/abhudia/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 391, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"/home/abhudia/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 113, in get\n    return _ForkingPickler.loads(res)\n  File \"stringsource\", line 5, in h5py.h5g.__pyx_unpickle_GroupID\n  File \"h5py/_objects.pyx\", line 178, in h5py._objects.ObjectID.__cinit__\nTypeError: __cinit__() takes exactly 1 positional argument (0 given)\n'''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8cc5aef1a8ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocity_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocity_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwater_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_currents_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnemoinput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mindicies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_currents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time elapsed: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeganat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "# Copyright 2013-2016 The Salish Sea MEOPAR contributors\n",
    "# and The University of British Columbia\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Creates forcing HDF5 input files for MOHID based on user input time range\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import errno\n",
    "import time\n",
    "import h5py\n",
    "from salishsea_tools import utilities\n",
    "from salishsea_tools import viz_tools\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#from scipy.interpolate import griddata\n",
    "\n",
    "# NEMO input files directory\n",
    "nemoinput = '/results2/SalishSea/nowcast-green.201806/'\n",
    "\n",
    "# HRDPS input files directory\n",
    "hdinput = '/results/forcing/atmospheric/GEM2.5/operational/'\n",
    "\n",
    "# WW3 input files directory\n",
    "wwinput = '/opp/wwatch3/nowcast/'\n",
    "\n",
    "# Output filepath\n",
    "outpath = '/results2/MIDOSS/forcing/SalishSeaCast/'\n",
    "\n",
    "\n",
    "## Integer -> String\n",
    "## consumes time in seconds and outputs a string that gives the time in HH:MM:SS format\n",
    "def conv_time(time):\n",
    "    \"\"\"Give time in HH:MM:SS format.\n",
    "\n",
    "    :arg time: time elapsed in seconds\n",
    "    :type integer: :py:class:'int'\n",
    "\n",
    "    :returns: time elapsed in HH:MM:SS format\n",
    "    :rtype: :py:class:`str'\n",
    "    \"\"\"\n",
    "    hours = int(time/3600)\n",
    "    mins = int((time - (hours*3600))/60)\n",
    "    secs = int((time - (3600 * hours) - (mins *60)))\n",
    "    return '{}:{}:{}'.format(hours, mins, secs)\n",
    "\n",
    "\n",
    "def generate_currents_hdf5(timestart, timeend, path, outpath, compression_level = 1):\n",
    "    \"\"\"Renerate current forcing HDF5 input file MOHID.\n",
    "\n",
    "    :arg timestart: date from when to start concatenating\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg timeend: date at which to stop concatenating\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg path: path of input files\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg outpath: path for output files\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg compression_level: compression level for output file (Integer[1,9])\n",
    "    :type integer: :py:class:'int'\n",
    "\n",
    "    :returns: None\n",
    "    :rtype: :py:class:`NoneType'\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate list of dates from daterange given\n",
    "    daterange = [parse(t) for t in [timestart, timeend]]\n",
    "    U_files = []\n",
    "    V_files = []\n",
    "    T_files = []\n",
    "\n",
    "    # append all filename strings within daterange to lists\n",
    "    for day in range(np.diff(daterange)[0].days):\n",
    "        datestamp = daterange[0] + timedelta(days=day)\n",
    "        datestr1 = datestamp.strftime('%d%b%y').lower()\n",
    "        datestr2 = datestamp.strftime('%Y%m%d')\n",
    "        \n",
    "        # check if file exists. exit if it does not. add path to list if it does.\n",
    "            # U files\n",
    "        U_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_U.nc'\n",
    "        if not os.path.exists(U_path):\n",
    "            print(f'File {U_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        U_files.append(U_path)\n",
    "            # V files\n",
    "        V_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_V.nc'\n",
    "        if not os.path.exists(V_path):\n",
    "            print(f'File {V_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        V_files.append(V_path)\n",
    "            # T files\n",
    "        T_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_T.nc'\n",
    "        if not os.path.exists(T_path):\n",
    "            print(f'File {T_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        T_files.append(T_path)\n",
    "        \n",
    "    print('\\nAll source files found')\n",
    "    # string: output folder name with date ranges used. end date will be lower by a day than timeend because datasets only go until midnight\n",
    "    folder = str(datetime(parse(timestart).year, parse(timestart).month, parse(timestart).day).strftime('%d%b%y').lower()) + '-' + str(datetime(parse(timeend).year, parse(timeend).month, parse(timeend).day-1).strftime('%d%b%y').lower())\n",
    "    # create output directory\n",
    "    dirname = f'{outpath}hdf5/{folder}/'\n",
    "    if not os.path.exists(os.path.dirname(dirname)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(dirname))\n",
    "        except OSError as exc:\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    print(f'\\nOutput directory {dirname} created\\n')\n",
    "    # create hdf5 file and create tree structure\n",
    "    f = h5py.File(f'{dirname}foocurrents.hdf5', 'w')\n",
    "    times = f.create_group('Time')\n",
    "    velocity_u = f.create_group('/Results/velocity U')\n",
    "    velocity_v = f.create_group('/Results/velocity V')\n",
    "    water_level = f.create_group('/Results/water level')\n",
    "    return f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level\n",
    "    \n",
    "#for file_index in bar(range(number_of_files)):\n",
    "def write_currents (file_index):\n",
    "    global U_files, V_files, T_files, f, times, velocity_u, velocity_v, water_level, compression_level\n",
    "    attr_counter = file_index * len(U_files)\n",
    "    U_raw = xr.open_dataset(U_files[file_index])\n",
    "    V_raw = xr.open_dataset(V_files[file_index])\n",
    "    T_raw = xr.open_dataset(T_files[file_index])\n",
    "    # assume all files have same time_counter markers\n",
    "    datelist = U_raw.time_counter.values.astype('datetime64[s]').astype(datetime)\n",
    "    # unstagger to move U, V to center of grid square\n",
    "    U  = viz_tools.unstagger_xarray(U_raw.vozocrtx, 'x')\n",
    "    V  = viz_tools.unstagger_xarray(V_raw.vomecrty, 'y')\n",
    "    # convert xarrays to numpy arrays and cut off grid edges\n",
    "    U = U.values[...,:,1:897:,1:397]\n",
    "    V = V.values[...,:,1:897:,1:397]\n",
    "    sea_surface = T_raw.sossheig.values[...,:,1:897:,1:397]\n",
    "    # rotate currents to True North\n",
    "    current_u, current_v = viz_tools.rotate_vel(U, V)\n",
    "    # clear memory\n",
    "    U, V = None, None\n",
    "    # transpose grid (rotate 90 clockwise)\n",
    "    current_u = np.transpose(current_u, [0,1,3,2])\n",
    "    current_v = np.transpose(current_v, [0,1,3,2])\n",
    "    sea_surface = np.transpose(sea_surface, [0,2,1])\n",
    "    # flip currents by depth dimension\n",
    "    current_u = np.flip(current_u, axis = 1)\n",
    "    current_v = np.flip(current_v, axis = 1)\n",
    "    # convert nans to 0's and set datatype to float64\n",
    "    current_u = np.nan_to_num(current_u).astype('float64')\n",
    "    current_v = np.nan_to_num(current_v).astype('float64')\n",
    "    sea_surface = np.nan_to_num(sea_surface).astype('float64')\n",
    "    # make list of time arrays\n",
    "    datearrays = []\n",
    "    for date in datelist:\n",
    "        datearrays.append(np.array([date.year, date.month, date.day, date.hour, date.minute, date.second]).astype('float64'))\n",
    "    # write u wind values to hdf5\n",
    "    for i in range(current_u.shape[0]):\n",
    "        velocity_attr = 'velocity U_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = velocity_u.create_dataset(velocity_attr, shape = (40, 396, 896), data = current_u[i],chunks=(40, 396, 896), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'FillValue' : np.array([0.]), 'Maximum' : np.array([5.]), 'Minimum' : np.array([-5.]), 'Units' : b'm/s'}\n",
    "        dset.attrs.update(metadata)\n",
    "\n",
    "    # write v wind values to hdf5\n",
    "    for i in range(current_v.shape[0]):\n",
    "        velocity_attr = 'velocity V_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = velocity_v.create_dataset(velocity_attr, shape = (40, 396, 896), data = current_v[i],chunks=(40, 396, 896), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'FillValue' : np.array([0.]), 'Maximum' : np.array([5.]), 'Minimum' : np.array([-5.]), 'Units' : b'm/s'}\n",
    "        dset.attrs.update(metadata)\n",
    "\n",
    "    # write  water level values to hdf5\n",
    "\n",
    "    for i in range(sea_surface.shape[0]):\n",
    "        level_attr = 'water level_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = water_level.create_dataset(level_attr, shape = (396, 896), data = sea_surface[i],chunks=(396, 896), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'FillValue' : np.array([0.]), 'Maximum' : np.array([5.]), 'Minimum' : np.array([-5.]), 'Units' : b'm'}\n",
    "        dset.attrs.update(metadata)\n",
    "\n",
    "    # write time values to hdf5\n",
    "\n",
    "    for i in range(len(datearrays)):\n",
    "        time_attr = 'Time_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = times.create_dataset(time_attr, shape = (6,), data = datearrays[i],chunks=(6,), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'Maximum' : np.array([datearrays[i][0].astype('float64')]), 'Minimum' : np.array([-0.]), 'Units' : b'YYYY/MM/DD HH:MM:SS'} # !!!\n",
    "        dset.attrs.update(metadata)\n",
    "    return\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    beganat = time.time()\n",
    "    timestart = '1 Feb 2019'\n",
    "    timeend = '8 Feb 2019'\n",
    "    f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level = generate_currents_hdf5(timestart, timeend, nemoinput, outpath, compression_level = 1)\n",
    "    indicies = [i for i in range(len(U_files))]\n",
    "    results = Parallel(n_jobs = 4)(delayed(write_currents)(i) for i in indicies)\n",
    "    f.close()\n",
    "    print('Time elapsed: {}'.format(conv_time(time.time()-beganat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    processes= []\n",
    "    #multiprocessing.set_start_method('spawn')\n",
    "    for i in range(len(U_files)):  \n",
    "        p = multiprocessing.Process(target = write_currents, args = (f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level, i))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    for process in processes:\n",
    "        process.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2013-2016 The Salish Sea MEOPAR contributors\n",
    "# and The University of British Columbia\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Creates forcing HDF5 input files for MOHID based on user input time range\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import errno\n",
    "import time\n",
    "import h5py\n",
    "from salishsea_tools import utilities\n",
    "from salishsea_tools import viz_tools\n",
    "import multiprocessing\n",
    "\n",
    "#from scipy.interpolate import griddata\n",
    "\n",
    "# NEMO input files directory\n",
    "nemoinput = '/results2/SalishSea/nowcast-green.201806/'\n",
    "\n",
    "# HRDPS input files directory\n",
    "hdinput = '/results/forcing/atmospheric/GEM2.5/operational/'\n",
    "\n",
    "# WW3 input files directory\n",
    "wwinput = '/opp/wwatch3/nowcast/'\n",
    "\n",
    "# Output filepath\n",
    "outpath = '/results2/MIDOSS/forcing/SalishSeaCast/'\n",
    "\n",
    "\n",
    "## Integer -> String\n",
    "## consumes time in seconds and outputs a string that gives the time in HH:MM:SS format\n",
    "def conv_time(time):\n",
    "    \"\"\"Give time in HH:MM:SS format.\n",
    "\n",
    "    :arg time: time elapsed in seconds\n",
    "    :type integer: :py:class:'int'\n",
    "\n",
    "    :returns: time elapsed in HH:MM:SS format\n",
    "    :rtype: :py:class:`str'\n",
    "    \"\"\"\n",
    "    hours = int(time/3600)\n",
    "    mins = int((time - (hours*3600))/60)\n",
    "    secs = int((time - (3600 * hours) - (mins *60)))\n",
    "    return '{}:{}:{}'.format(hours, mins, secs)\n",
    "\n",
    "\n",
    "def generate_currents_hdf5(timestart, timeend, path, outpath, compression_level = 1):\n",
    "    \"\"\"Renerate current forcing HDF5 input file MOHID.\n",
    "\n",
    "    :arg timestart: date from when to start concatenating\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg timeend: date at which to stop concatenating\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg path: path of input files\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg outpath: path for output files\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg compression_level: compression level for output file (Integer[1,9])\n",
    "    :type integer: :py:class:'int'\n",
    "\n",
    "    :returns: None\n",
    "    :rtype: :py:class:`NoneType'\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate list of dates from daterange given\n",
    "    daterange = [parse(t) for t in [timestart, timeend]]\n",
    "    U_files = []\n",
    "    V_files = []\n",
    "    T_files = []\n",
    "\n",
    "    # append all filename strings within daterange to lists\n",
    "    for day in range(np.diff(daterange)[0].days):\n",
    "        datestamp = daterange[0] + timedelta(days=day)\n",
    "        datestr1 = datestamp.strftime('%d%b%y').lower()\n",
    "        datestr2 = datestamp.strftime('%Y%m%d')\n",
    "        \n",
    "        # check if file exists. exit if it does not. add path to list if it does.\n",
    "            # U files\n",
    "        U_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_U.nc'\n",
    "        if not os.path.exists(U_path):\n",
    "            print(f'File {U_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        U_files.append(U_path)\n",
    "            # V files\n",
    "        V_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_V.nc'\n",
    "        if not os.path.exists(V_path):\n",
    "            print(f'File {V_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        V_files.append(V_path)\n",
    "            # T files\n",
    "        T_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_T.nc'\n",
    "        if not os.path.exists(T_path):\n",
    "            print(f'File {T_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        T_files.append(T_path)\n",
    "        \n",
    "    print('\\nAll source files found')\n",
    "    # string: output folder name with date ranges used. end date will be lower by a day than timeend because datasets only go until midnight\n",
    "    folder = str(datetime(parse(timestart).year, parse(timestart).month, parse(timestart).day).strftime('%d%b%y').lower()) + '-' + str(datetime(parse(timeend).year, parse(timeend).month, parse(timeend).day-1).strftime('%d%b%y').lower())\n",
    "    # create output directory\n",
    "    dirname = f'{outpath}hdf5/{folder}/'\n",
    "    if not os.path.exists(os.path.dirname(dirname)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(dirname))\n",
    "        except OSError as exc:\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    print(f'\\nOutput directory {dirname} created\\n')\n",
    "    # create hdf5 file and create tree structure\n",
    "    f = h5py.File(f'{dirname}foocurrents.hdf5', 'w')\n",
    "    times = f.create_group('Time')\n",
    "    velocity_u = f.create_group('/Results/velocity U')\n",
    "    velocity_v = f.create_group('/Results/velocity V')\n",
    "    water_level = f.create_group('/Results/water level')\n",
    "    return f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level\n",
    "    \n",
    "#for file_index in bar(range(number_of_files)):\n",
    "def write_currents (f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level, file_index):\n",
    "    attr_counter = file_index * len(U_files)\n",
    "    U_raw = xr.open_dataset(U_files[file_index])\n",
    "    V_raw = xr.open_dataset(V_files[file_index])\n",
    "    T_raw = xr.open_dataset(T_files[file_index])\n",
    "    # assume all files have same time_counter markers\n",
    "    datelist = U_raw.time_counter.values.astype('datetime64[s]').astype(datetime)\n",
    "    # unstagger to move U, V to center of grid square\n",
    "    U  = viz_tools.unstagger_xarray(U_raw.vozocrtx, 'x')\n",
    "    V  = viz_tools.unstagger_xarray(V_raw.vomecrty, 'y')\n",
    "    # convert xarrays to numpy arrays and cut off grid edges\n",
    "    U = U.values[...,:,1:897:,1:397]\n",
    "    V = V.values[...,:,1:897:,1:397]\n",
    "    sea_surface = T_raw.sossheig.values[...,:,1:897:,1:397]\n",
    "    # rotate currents to True North\n",
    "    current_u, current_v = viz_tools.rotate_vel(U, V)\n",
    "    # clear memory\n",
    "    U, V = None, None\n",
    "    # transpose grid (rotate 90 clockwise)\n",
    "    current_u = np.transpose(current_u, [0,1,3,2])\n",
    "    current_v = np.transpose(current_v, [0,1,3,2])\n",
    "    sea_surface = np.transpose(sea_surface, [0,2,1])\n",
    "    # flip currents by depth dimension\n",
    "    current_u = np.flip(current_u, axis = 1)\n",
    "    current_v = np.flip(current_v, axis = 1)\n",
    "    # convert nans to 0's and set datatype to float64\n",
    "    current_u = np.nan_to_num(current_u).astype('float64')\n",
    "    current_v = np.nan_to_num(current_v).astype('float64')\n",
    "    sea_surface = np.nan_to_num(sea_surface).astype('float64')\n",
    "    # make list of time arrays\n",
    "    datearrays = []\n",
    "    for date in datelist:\n",
    "        datearrays.append(np.array([date.year, date.month, date.day, date.hour, date.minute, date.second]).astype('float64'))\n",
    "    # write u wind values to hdf5\n",
    "    for i in range(current_u.shape[0]):\n",
    "        velocity_attr = 'velocity U_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = velocity_u.create_dataset(velocity_attr, shape = (40, 396, 896), data = current_u[i],chunks=(40, 396, 896), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'FillValue' : np.array([0.]), 'Maximum' : np.array([5.]), 'Minimum' : np.array([-5.]), 'Units' : b'm/s'}\n",
    "        dset.attrs.update(metadata)\n",
    "\n",
    "    # write v wind values to hdf5\n",
    "    for i in range(current_v.shape[0]):\n",
    "        velocity_attr = 'velocity V_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = velocity_v.create_dataset(velocity_attr, shape = (40, 396, 896), data = current_v[i],chunks=(40, 396, 896), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'FillValue' : np.array([0.]), 'Maximum' : np.array([5.]), 'Minimum' : np.array([-5.]), 'Units' : b'm/s'}\n",
    "        dset.attrs.update(metadata)\n",
    "\n",
    "    # write  water level values to hdf5\n",
    "\n",
    "    for i in range(sea_surface.shape[0]):\n",
    "        level_attr = 'water level_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = water_level.create_dataset(level_attr, shape = (396, 896), data = sea_surface[i],chunks=(396, 896), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'FillValue' : np.array([0.]), 'Maximum' : np.array([5.]), 'Minimum' : np.array([-5.]), 'Units' : b'm'}\n",
    "        dset.attrs.update(metadata)\n",
    "\n",
    "    # write time values to hdf5\n",
    "\n",
    "    for i in range(len(datearrays)):\n",
    "        time_attr = 'Time_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = times.create_dataset(time_attr, shape = (6,), data = datearrays[i],chunks=(6,), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'Maximum' : np.array([datearrays[i][0].astype('float64')]), 'Minimum' : np.array([-0.]), 'Units' : b'YYYY/MM/DD HH:MM:SS'} # !!!\n",
    "        dset.attrs.update(metadata)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    beganat = time.time()\n",
    "    timestart = '08 Feb 2019'\n",
    "    timeend = '10 Feb 2019'\n",
    "    f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level = generate_currents_hdf5(timestart, timeend, nemoinput, outpath, compression_level = 1)\n",
    "    processes= []\n",
    "    #multiprocessing.set_start_method('spawn')\n",
    "    for i in range(len(U_files)):  \n",
    "        p = multiprocessing.Process(target = write_currents, args = (f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level, i))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    f.close()\n",
    "    print('Time elapsed: {}'.format(conv_time(time.time()-beganat)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
