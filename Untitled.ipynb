{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tables import* \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) ''\n",
      "/Results (Group) ''\n",
      "/Time (Group) ''\n",
      "/Results/velocity U (Group) ''\n",
      "/Results/velocity V (Group) ''\n",
      "/Results/water level (Group) ''\n"
     ]
    }
   ],
   "source": [
    "h5wind = open_file('/results2/MIDOSS/forcing/SalishSeaCast/hdf5/01feb19-07feb19/foocurrents.hdf5')\n",
    "for group in h5wind.walk_groups():\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/Results/velocity U (Group) ''\n",
       "  children := []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(h5wind.root.Results, 'velocity U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All source files found\n",
      "\n",
      "Output directory /results2/MIDOSS/forcing/SalishSeaCast/hdf5/01feb19-07feb19/ created\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2013-2016 The Salish Sea MEOPAR contributors\n",
    "# and The University of British Columbia\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Creates forcing HDF5 input files for MOHID based on user input time range\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import errno\n",
    "import time\n",
    "import h5py\n",
    "from salishsea_tools import utilities\n",
    "from salishsea_tools import viz_tools\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#from scipy.interpolate import griddata\n",
    "\n",
    "# NEMO input files directory\n",
    "nemoinput = '/results2/SalishSea/nowcast-green.201806/'\n",
    "\n",
    "# HRDPS input files directory\n",
    "hdinput = '/results/forcing/atmospheric/GEM2.5/operational/'\n",
    "\n",
    "# WW3 input files directory\n",
    "wwinput = '/opp/wwatch3/nowcast/'\n",
    "\n",
    "# Output filepath\n",
    "outpath = '/results2/MIDOSS/forcing/SalishSeaCast/'\n",
    "\n",
    "\n",
    "## Integer -> String\n",
    "## consumes time in seconds and outputs a string that gives the time in HH:MM:SS format\n",
    "def conv_time(time):\n",
    "    \"\"\"Give time in HH:MM:SS format.\n",
    "\n",
    "    :arg time: time elapsed in seconds\n",
    "    :type integer: :py:class:'int'\n",
    "\n",
    "    :returns: time elapsed in HH:MM:SS format\n",
    "    :rtype: :py:class:`str'\n",
    "    \"\"\"\n",
    "    hours = int(time/3600)\n",
    "    mins = int((time - (hours*3600))/60)\n",
    "    secs = int((time - (3600 * hours) - (mins *60)))\n",
    "    return '{}:{}:{}'.format(hours, mins, secs)\n",
    "\n",
    "\n",
    "def generate_currents_hdf5(timestart, timeend, path, outpath, compression_level = 1):\n",
    "    \"\"\"Provide paths, groups and parameters for multiprocessing\n",
    "\n",
    "    :arg timestart: date from when to start concatenating\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg timeend: date at which to stop concatenating\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg path: path of input files\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg outpath: path for output files\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg compression_level: compression level for output file (Integer[1,9])\n",
    "    :type integer: :py:class:'int'\n",
    "\n",
    "    :returns f: <HDF5 file (mode r+)>\n",
    "    :rtype: :py:class:`File'\n",
    "    \n",
    "    :returns U_files: listofString: U parameter file paths\n",
    "    :rtype: :py:class:`list'\n",
    "    \n",
    "    :returns V_files: listofString: V parameter file paths\n",
    "    :rtype: :py:class:`list'\n",
    "    \n",
    "    :returns T_files: listofString: T parameter file paths\n",
    "    :rtype: :py:class:`list'\n",
    "    \n",
    "    :returns times: HDF5 group for time\n",
    "    :rtype: :py:class:`Group'\n",
    "    \n",
    "    :returns velocity_u: HDF5 group for U velocities\n",
    "    :rtype: :py:class:`Group'\n",
    "    \n",
    "    :returns velocity_v: HDF5 group for V velocities\n",
    "    :rtype: :py:class:`Group'\n",
    "    \n",
    "    :returns water_level: HDF5 group for sea surface heights\n",
    "    :rtype: :py:class:`Group'\n",
    "\n",
    "    :returns compression_level: compression level for output file (Integer[1,9])\n",
    "    :type integer: :py:class:'int'\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate list of dates from daterange given\n",
    "    daterange = [parse(t) for t in [timestart, timeend]]\n",
    "    U_files = []\n",
    "    V_files = []\n",
    "    T_files = []\n",
    "\n",
    "    # append all filename strings within daterange to lists\n",
    "    for day in range(np.diff(daterange)[0].days):\n",
    "        datestamp = daterange[0] + timedelta(days=day)\n",
    "        datestr1 = datestamp.strftime('%d%b%y').lower()\n",
    "        datestr2 = datestamp.strftime('%Y%m%d')\n",
    "        \n",
    "        # check if file exists. exit if it does not. add path to list if it does.\n",
    "            # U files\n",
    "        U_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_U.nc'\n",
    "        if not os.path.exists(U_path):\n",
    "            print(f'File {U_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        U_files.append(U_path)\n",
    "            # V files\n",
    "        V_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_V.nc'\n",
    "        if not os.path.exists(V_path):\n",
    "            print(f'File {V_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        V_files.append(V_path)\n",
    "            # T files\n",
    "        T_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_T.nc'\n",
    "        if not os.path.exists(T_path):\n",
    "            print(f'File {T_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        T_files.append(T_path)\n",
    "        \n",
    "    print('\\nAll source files found')\n",
    "    # string: output folder name with date ranges used. end date will be lower by a day than timeend because datasets only go until midnight\n",
    "    folder = str(datetime(parse(timestart).year, parse(timestart).month, parse(timestart).day).strftime('%d%b%y').lower()) + '-' + str(datetime(parse(timeend).year, parse(timeend).month, parse(timeend).day-1).strftime('%d%b%y').lower())\n",
    "    # create output directory\n",
    "    dirname = f'{outpath}hdf5/{folder}/'\n",
    "    if not os.path.exists(os.path.dirname(dirname)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(dirname))\n",
    "        except OSError as exc:\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    print(f'\\nOutput directory {dirname} created\\n')\n",
    "    # create hdf5 file and create tree structure\n",
    "    f = h5py.File(f'{dirname}foocurrents.hdf5', 'w')\n",
    "    times = f.create_group('Time')\n",
    "    velocity_u = f.create_group('/Results/velocity U')\n",
    "    velocity_v = f.create_group('/Results/velocity V')\n",
    "    water_level = f.create_group('/Results/water level')\n",
    "    return f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    beganat = time.time()\n",
    "    timestart = '1 Feb 2019'\n",
    "    timeend = '8 Feb 2019'\n",
    "    f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level = generate_currents_hdf5(timestart, timeend, nemoinput, outpath, compression_level = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/Time\" (0 members)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2013-2016 The Salish Sea MEOPAR contributors\n",
    "# and The University of British Columbia\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Creates forcing HDF5 input files for MOHID based on user input time range\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import errno\n",
    "import time\n",
    "import h5py\n",
    "from salishsea_tools import utilities\n",
    "from salishsea_tools import viz_tools\n",
    "import multiprocessing\n",
    "\n",
    "#from scipy.interpolate import griddata\n",
    "\n",
    "# NEMO input files directory\n",
    "nemoinput = '/results2/SalishSea/nowcast-green.201806/'\n",
    "\n",
    "# HRDPS input files directory\n",
    "hdinput = '/results/forcing/atmospheric/GEM2.5/operational/'\n",
    "\n",
    "# WW3 input files directory\n",
    "wwinput = '/opp/wwatch3/nowcast/'\n",
    "\n",
    "# Output filepath\n",
    "outpath = '/results2/MIDOSS/forcing/SalishSeaCast/'\n",
    "\n",
    "\n",
    "## Integer -> String\n",
    "## consumes time in seconds and outputs a string that gives the time in HH:MM:SS format\n",
    "def conv_time(time):\n",
    "    \"\"\"Give time in HH:MM:SS format.\n",
    "\n",
    "    :arg time: time elapsed in seconds\n",
    "    :type integer: :py:class:'int'\n",
    "\n",
    "    :returns: time elapsed in HH:MM:SS format\n",
    "    :rtype: :py:class:`str'\n",
    "    \"\"\"\n",
    "    hours = int(time/3600)\n",
    "    mins = int((time - (hours*3600))/60)\n",
    "    secs = int((time - (3600 * hours) - (mins *60)))\n",
    "    return '{}:{}:{}'.format(hours, mins, secs)\n",
    "\n",
    "\n",
    "def generate_currents_hdf5(timestart, timeend, path, outpath, compression_level = 1):\n",
    "    \"\"\"Renerate current forcing HDF5 input file MOHID.\n",
    "\n",
    "    :arg timestart: date from when to start concatenating\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg timeend: date at which to stop concatenating\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg path: path of input files\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg outpath: path for output files\n",
    "    :type string: :py:class:'str'\n",
    "\n",
    "    :arg compression_level: compression level for output file (Integer[1,9])\n",
    "    :type integer: :py:class:'int'\n",
    "\n",
    "    :returns: None\n",
    "    :rtype: :py:class:`NoneType'\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate list of dates from daterange given\n",
    "    daterange = [parse(t) for t in [timestart, timeend]]\n",
    "    U_files = []\n",
    "    V_files = []\n",
    "    T_files = []\n",
    "\n",
    "    # append all filename strings within daterange to lists\n",
    "    for day in range(np.diff(daterange)[0].days):\n",
    "        datestamp = daterange[0] + timedelta(days=day)\n",
    "        datestr1 = datestamp.strftime('%d%b%y').lower()\n",
    "        datestr2 = datestamp.strftime('%Y%m%d')\n",
    "        \n",
    "        # check if file exists. exit if it does not. add path to list if it does.\n",
    "            # U files\n",
    "        U_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_U.nc'\n",
    "        if not os.path.exists(U_path):\n",
    "            print(f'File {U_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        U_files.append(U_path)\n",
    "            # V files\n",
    "        V_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_V.nc'\n",
    "        if not os.path.exists(V_path):\n",
    "            print(f'File {V_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        V_files.append(V_path)\n",
    "            # T files\n",
    "        T_path = f'{path}{datestr1}/SalishSea_1h_{datestr2}_{datestr2}_grid_T.nc'\n",
    "        if not os.path.exists(T_path):\n",
    "            print(f'File {T_path} not found. Check Directory and/or Date Range.')\n",
    "            return\n",
    "        T_files.append(T_path)\n",
    "        \n",
    "    print('\\nAll source files found')\n",
    "    # string: output folder name with date ranges used. end date will be lower by a day than timeend because datasets only go until midnight\n",
    "    folder = str(datetime(parse(timestart).year, parse(timestart).month, parse(timestart).day).strftime('%d%b%y').lower()) + '-' + str(datetime(parse(timeend).year, parse(timeend).month, parse(timeend).day-1).strftime('%d%b%y').lower())\n",
    "    # create output directory\n",
    "    dirname = f'{outpath}hdf5/{folder}/'\n",
    "    if not os.path.exists(os.path.dirname(dirname)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(dirname))\n",
    "        except OSError as exc:\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    print(f'\\nOutput directory {dirname} created\\n')\n",
    "    # create hdf5 file and create tree structure\n",
    "    f = h5py.File(f'{dirname}foocurrents.hdf5', 'w')\n",
    "    times = f.create_group('Time')\n",
    "    velocity_u = f.create_group('/Results/velocity U')\n",
    "    velocity_v = f.create_group('/Results/velocity V')\n",
    "    water_level = f.create_group('/Results/water level')\n",
    "    return f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level\n",
    "    \n",
    "#for file_index in bar(range(number_of_files)):\n",
    "def write_currents (f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level, file_index):\n",
    "    attr_counter = file_index * len(U_files)\n",
    "    U_raw = xr.open_dataset(U_files[file_index])\n",
    "    V_raw = xr.open_dataset(V_files[file_index])\n",
    "    T_raw = xr.open_dataset(T_files[file_index])\n",
    "    # assume all files have same time_counter markers\n",
    "    datelist = U_raw.time_counter.values.astype('datetime64[s]').astype(datetime)\n",
    "    # unstagger to move U, V to center of grid square\n",
    "    U  = viz_tools.unstagger_xarray(U_raw.vozocrtx, 'x')\n",
    "    V  = viz_tools.unstagger_xarray(V_raw.vomecrty, 'y')\n",
    "    # convert xarrays to numpy arrays and cut off grid edges\n",
    "    U = U.values[...,:,1:897:,1:397]\n",
    "    V = V.values[...,:,1:897:,1:397]\n",
    "    sea_surface = T_raw.sossheig.values[...,:,1:897:,1:397]\n",
    "    # rotate currents to True North\n",
    "    current_u, current_v = viz_tools.rotate_vel(U, V)\n",
    "    # clear memory\n",
    "    U, V = None, None\n",
    "    # transpose grid (rotate 90 clockwise)\n",
    "    current_u = np.transpose(current_u, [0,1,3,2])\n",
    "    current_v = np.transpose(current_v, [0,1,3,2])\n",
    "    sea_surface = np.transpose(sea_surface, [0,2,1])\n",
    "    # flip currents by depth dimension\n",
    "    current_u = np.flip(current_u, axis = 1)\n",
    "    current_v = np.flip(current_v, axis = 1)\n",
    "    # convert nans to 0's and set datatype to float64\n",
    "    current_u = np.nan_to_num(current_u).astype('float64')\n",
    "    current_v = np.nan_to_num(current_v).astype('float64')\n",
    "    sea_surface = np.nan_to_num(sea_surface).astype('float64')\n",
    "    # make list of time arrays\n",
    "    datearrays = []\n",
    "    for date in datelist:\n",
    "        datearrays.append(np.array([date.year, date.month, date.day, date.hour, date.minute, date.second]).astype('float64'))\n",
    "    # write u wind values to hdf5\n",
    "    for i in range(current_u.shape[0]):\n",
    "        velocity_attr = 'velocity U_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = velocity_u.create_dataset(velocity_attr, shape = (40, 396, 896), data = current_u[i],chunks=(40, 396, 896), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'FillValue' : np.array([0.]), 'Maximum' : np.array([5.]), 'Minimum' : np.array([-5.]), 'Units' : b'm/s'}\n",
    "        dset.attrs.update(metadata)\n",
    "\n",
    "    # write v wind values to hdf5\n",
    "    for i in range(current_v.shape[0]):\n",
    "        velocity_attr = 'velocity V_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = velocity_v.create_dataset(velocity_attr, shape = (40, 396, 896), data = current_v[i],chunks=(40, 396, 896), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'FillValue' : np.array([0.]), 'Maximum' : np.array([5.]), 'Minimum' : np.array([-5.]), 'Units' : b'm/s'}\n",
    "        dset.attrs.update(metadata)\n",
    "\n",
    "    # write  water level values to hdf5\n",
    "\n",
    "    for i in range(sea_surface.shape[0]):\n",
    "        level_attr = 'water level_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = water_level.create_dataset(level_attr, shape = (396, 896), data = sea_surface[i],chunks=(396, 896), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'FillValue' : np.array([0.]), 'Maximum' : np.array([5.]), 'Minimum' : np.array([-5.]), 'Units' : b'm'}\n",
    "        dset.attrs.update(metadata)\n",
    "\n",
    "    # write time values to hdf5\n",
    "\n",
    "    for i in range(len(datearrays)):\n",
    "        time_attr = 'Time_' + ((5 - len(str(i + attr_counter + 1))) * '0') + str(i + attr_counter + 1)\n",
    "        dset = times.create_dataset(time_attr, shape = (6,), data = datearrays[i],chunks=(6,), compression = 'gzip', compression_opts = compression_level)\n",
    "        metadata = {'Maximum' : np.array([datearrays[i][0].astype('float64')]), 'Minimum' : np.array([-0.]), 'Units' : b'YYYY/MM/DD HH:MM:SS'} # !!!\n",
    "        dset.attrs.update(metadata)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    beganat = time.time()\n",
    "    timestart = '08 Feb 2019'\n",
    "    timeend = '10 Feb 2019'\n",
    "    f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level = generate_currents_hdf5(timestart, timeend, nemoinput, outpath, compression_level = 1)\n",
    "    processes= []\n",
    "    #multiprocessing.set_start_method('spawn')\n",
    "    for i in range(len(U_files)):  \n",
    "        p = multiprocessing.Process(target = write_currents, args = (f, U_files, V_files, T_files, times, velocity_u, velocity_v, water_level, compression_level, i))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    f.close()\n",
    "    print('Time elapsed: {}'.format(conv_time(time.time()-beganat)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
