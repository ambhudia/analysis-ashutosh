19 Feb 2019

    Replicate Shihan's run using files from nextcloud
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    Purpose: To determine of the model was chnaged since the intial run

        Download files from Nextcloud, upload to cedar
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            Create directory (/home/abhudia/project/abhudia/MIDOSS/MIDOSS-MOHID-config/ashu_testing/nextcloud_replicate)
            Write .yaml file pointing to all relevant input files
            Edit file paths in Atmosphere.dat, Hydrodynamic.dat

        Compile Mohid
        ^^^^^^^^^^^^^
            salloc --time=0:10:0 --cpus-per-task=1 --mem-per-cpu=1024m --account=def-allen
            cd $PROJECT/$USER/MIDOSS/MIDOSS-MOHID/Solutions/linux
            ./compile_mohid.sh -mb1 -mb2 -mw

            Delete all of the compiled objects, libraries, and executables:
            ./compile_mohid --clean
            so that next build is clean
        
        Run Mohid
        ^^^^^^^^^
            make run directory as set in yaml file ($SCRATCH/MIDOSS/runs/ashu_testing/)
            cd /home/abhudia/project/abhudia/MIDOSS/MIDOSS-MOHID-config/ashu_testing/nextcloud_replicate
            mohid run nextcloud_replicate.yaml $PROJECT/$USER/MIDOSS/results/ashu/nextcloud_replicate

        Outcome
        ^^^^^^^
            crashed with multiple error messages. will investigate.

20 Feb 2019

    Continue developing scripts to generate hdf5 input files for MOHID
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    
        Added w ocean velocity to input file generator function
        !! need to add date validation on input
        !! need to create local file for lat lon data for HRDPS and WW3 grids
    
    Replicate Shihan's run using files from nextcloud
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    Purpose: To determine of the model was chnaged since the intial run

        Download files from Nextcloud, upload to cedar
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            Create directory (/home/abhudia/project/abhudia/MIDOSS/MIDOSS-MOHID-config/ashu_testing/nextcloud_replicate)
            Write .yaml file pointing to all relevant input files
            Edit file paths in Atmosphere.dat, Hydrodynamic.dat

        Compile Mohid
        ^^^^^^^^^^^^^
            salloc --time=0:10:0 --cpus-per-task=1 --mem-per-cpu=1024m --account=def-allen
            cd $PROJECT/$USER/MIDOSS/MIDOSS-MOHID/Solutions/linux
            ./compile_mohid.sh -mb1 -mb2 -mw

            Delete all of the compiled objects, libraries, and executables:
            ./compile_mohid --clean
            so that next build is clean
        
        Run Mohid
        ^^^^^^^^^
            make run directory as set in yaml file ($SCRATCH/MIDOSS/runs/ashu_testing/)
            cd /home/abhudia/project/abhudia/MIDOSS/MIDOSS-MOHID-config/ashu_testing/nextcloud_replicate
            mohid run nextcloud_replicate.yaml $PROJECT/$USER/MIDOSS/results/ashu/nextcloud_replicate
            Job ID: 17103921
        
        Outcome
        ^^^^^^^
            crashed again. stderr output:
                ReadInitialImposedSolution  - ModuleHydrodynamic - ERR170
                Usage: hdf5-to-netcdf4 [OPTIONS] HDF5_FILE NETCDF4_FILE
                Try "hdf5-to-netcdf4 --help" for help.

                Error: Invalid value for "HDF5_FILE": Path "/scratch/abhudia/MIDOSS/runs/ashu_testing/nextcloud_replicate_2019-02-20T171143.680870-0800/res/Lagrangian_nextcloud_replicate.hdf5" does not exist.


21 Feb 2019

    Continue developing script to generate hdf5 input files for MOHID
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    
        added salinity and temperature
        refactored script to generate file paths and produce hdf5 files in separate functions. added additional validation.
    
    Testing Script on Salish
    ^^^^^^^^^^^^^^^^^^^^^^^^

        SalishSeacast output directory /results2/MIDOSS/forcing/SalishSeaCast/ashu_testing/hdf5/01oct18-07oct18/ created
        HRDPS output directory /results2/MIDOSS/forcing/SalishSeaCast/ashu_testing/hrdps/01oct18-07oct18/ created
        WW3 output directory /results2/MIDOSS/forcing/SalishSeaCast/ashu_testing/ww3/01oct18-07oct18/ created

        Crashed on WW3. Output:
        Creating WW3 parameters file ... N/A% (0 of 7) |                                                          |ETA:  --:--:--Traceback (most recent call last):
        File "<stdin>", line 1, in <module>
        File "make-hdf5.py", line 1013, in init
            run_choice()
        File "make-hdf5.py", line 949, in run_choice
            run(runs)
        File "make-hdf5.py", line 48, in f
            rv      = func(*args, *kwargs)
        File "make-hdf5.py", line 972, in run
            create_ww3_hdf5(*ww3)
        File "make-hdf5.py", line 48, in f
            rv      = func(*args, *kwargs)
        File "make-hdf5.py", line 790, in create_ww3_hdf5
            mean_wave = np.expand_dims(griddata(points, mean_wave_array[0].ravel(), xi, method='cubic'),0)
        File "/home/abhudia/anaconda3/lib/python3.6/site-packages/scipy/interpolate/ndgriddata.py", line 206, in griddata
            raise ValueError("invalid number of dimensions in xi")
        ValueError: invalid number of dimensions in xi
        >>> mean_wave_array
        Traceback (most recent call last):
        File "<stdin>", line 1, in <module>
        NameError: name 'mean_wave_array' is not defined
