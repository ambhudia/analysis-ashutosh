5 March 2019
^^^^^^^^^^^^
    Fixing the hdf5 > netcdf conversion issue
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        $ cd
        $ rm -rf .local
        $ cd $PROJECT/$USER/MIDOSS/
        $ pip3 install --user -e moad_tools
        $ hdf5-to-netcdf4 --help
        $ pip install --user -e NEMO-Cmd/
        $ pip install --user -e MOHID-Cmd/

    Setting up to make multiple runs at SoG point
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        From Rachael's spreadsheet document, I have set my script to generate the hdf5 input files over five different time ranges.
        The conversion is being done on hake.

        On Cedar
        ^^^^^^^^
        Created runs config directory: /home/abhudia/project/abhudia/MIDOSS/MIDOSS-MOHID-config/MF0

        This directory contains:
        settings: frequently used .dat files that do not change
                Atmosphere.dat    InterfaceSedimentWater.dat              Tide.dat
                Geometry.dat      InterfaceWaterAir.dat                   Turbulence.dat
                Hydrodynamic.dat  ST_georgia_bathymetry_modified_v02.dat  WaterProperties.dat
        folders with run IDS that have input hdf5 files, .yaml, Lagranigan.dat, Model.dat and Waves.dat
                .yaml file header gives job name and allocates resources to job. contains the locations of all the dat files and instructions on creating symlinks in the runs dir.
                Lagrangian.dat used primarily to chnage location of Oil spill. This has been updated with the Medium Floater attributes from the oil types spreadsheet.
                Model.dat used to set the time period over which we want MOHID to run
                Waves.dat will contain the required information on how to deal with waves depending on whether or not we have the ww3.hdf5 file for a run

        What changes from job to job
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        1) .yaml: the job ID in the header, wall time depending on how long we are running + how many input parameters we have
        2) Model.dat: the model run duration
        3) Waves.dat: depending on whether or not we have wave watch input data
        4) Lagrangian.dat: the oil spill location/oil type

    Making test run
    ^^^^^^^^^^^^^^^
        A test run was made on cedar with all the forcing parameters and a reduced model run time. Successful.
        Wave file was output in the res directory. This was fixed by making the hdf5 out 0 in the Waves.dat file

6 March 2019
^^^^^^^^^^^^
    Setting up to make multiple runs at the Strait of Georgia point
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        1) To get a feel of how long a job is likely to take, I have set the SOG12117 case to run for the full duration of the input files on a wall time of 1:50:00
            
            [abhudia@cedar1 ~]$ cd project/abhudia/MIDOSS/MIDOSS-MOHID-config/MF0/SOG12117/
            [abhudia@cedar1 SOG12117]$ mohid run SOG12117.yaml $PROJECT/$USER/MIDOSS/results/MF0/SOG12117

        ConstructHDFPredictDTMethod1 - ModuleFillMatrix - ERR50
        Could not read solution from HDF5 file
        Last instant in file lower than simulation ending time
        Matrix name: wind velocity X
        
        Shihan sent over new bathymetry file. I am switching to that one for this initial run.

            THE RUN JUST ENDED: NOTE YOU NEED TO RE RUN AFTER INPUTTING NEW BATHYMETRY 
         A new Bathymetry has been created, which consists with the geometry
         Modify the file Nomfich.dat and Re-run the model


        2) In the meantime, I am copying over the other job files to Cedar and making the required chnages to each job's YAML file and .dat files.
                SOG060515_7 : set model.dat date range
                Set current run attributes in YAML file and pointed to Waves.dat in settings since no ww3 data
                



        3) I am also getting the turn point input files going on Salish


        
