{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 File Creation and Conventions Documentation\n",
    "\n",
    "#### Based on my experience with using the __[h5py](http://docs.h5py.org/en/stable/)__ library to create forcing files for MOHID, this notebook documents the recommended way of creating HDF5 files with a tree structure, compression variables and metadata attributes for datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating HDF5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code that follows assumes that the `h5py` python library has been imported:\n",
    "    \n",
    "```python\n",
    "import h5py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an empty HDF5 file. Creating an instance of a  `File` object gives the HDF5 file a `root` or `/` Group and returns a 'File' object which can be assigned to a variable. The code that follows assumes that the `foo` file object is open. \n",
    "\n",
    "```python\n",
    "foo = h5py.File('foo.hdf5', 'w')\n",
    "```\n",
    "#### The first argument that `File` takes is the path and name of the `.hdf5` file that will be created, updated or read. The second argument is the mode with which to access the file. From the __[h5py docmunetation](http://docs.h5py.org/en/stable/high/file.html)__:\n",
    "| Mode | What it does |\n",
    "| --- | --- |\n",
    "| 'r' | Read only, file must exist |\n",
    "| 'r+' | Read/write, file must exist |\n",
    "| 'w' | Create file, truncate if exists |\n",
    "| 'w-' or x | Create file, fail if exists |\n",
    "| 'a' | Read/write if exists, create otherwise (default) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Tree Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The way MOHID reads in the hdf5 files requires the datasets to be contianed systematically in an arbitrary arity tree (__[from CPSC110](https://eyqs.ca/assets/documents/UBCCPSC110.txt)__: a tree whose nodes have an arbitrary number of children)\n",
    "\n",
    "#### The tree structure is comprised of:\n",
    "#### - Groups: The containers that create the 'nodes'/'branches' of the tree that hold the datasets ('children'/'leaves')\n",
    "#### - Datasets: Homogeneous collections (i.e. all elements are of one type, such as `float64`, `int`, and so on) of data. These are the 'children'/'leaves' of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computer scientists draw their trees upside down\n",
    "#### Consider the example:\n",
    "                            foo.hdf5 (root or /)\n",
    "                                |\n",
    "                                |\n",
    "               +------------------------------------+\n",
    "               |                                    |\n",
    "               |                                    |\n",
    "             time (Group)                        Results (Group)\n",
    "              - time_00001 (Dataset)                 |\n",
    "              - time_00002 (Dataset)                 |\n",
    "                                                     |\n",
    "                                         +-----------------------+\n",
    "                                         |                       |\n",
    "                                         |                       |\n",
    "                                      bar (Group)            baz (Group)\n",
    "                                       - bar_00001 (Dataset)  - baz_00001 (Dataset)\n",
    "                                       - bar_00002 (Dataset)  - baz_00002 (Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can have nested Groups, such as `bar` and `baz`, which are children of the `Results` Group\n",
    "#### The tree structure terminates with datasets (You could terminate with another Group, but that would just be an empty container\n",
    "#### All children at a certain level must have unique names, for instance we cannot have two Datasets under `time` called `time_00001` or two groups under `Results` called `bar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group objects are created using the method `Group.create_group()`, where `Group` is a Group object. This method works on a newly created file object like `foo` because the file was created by default with a `root` or `/` Group. The `Group.create_group()` method takes a `name` argument, which is an identifier for the group, and returns a Group object that can be assigned to a variable. You can read more about Groups __[here](http://docs.h5py.org/en/stable/high/group.html#creating-groups)__\n",
    "\n",
    "#### Create a Group on `root` or `/`\n",
    "##### This creates \"/Results\" explicitly\n",
    "\n",
    "```python\n",
    "Results = foo.create_group('Results')\n",
    "```\n",
    "\n",
    "\n",
    "#### Create a nested Group by creating a new child group on an existing group\n",
    "##### This creates \"/Results/bar\" explicitly\n",
    "\n",
    "```python\n",
    "bar = Results.create_group('bar')\n",
    "```\n",
    "\n",
    "\n",
    "#### Implicitly define a Group, for instance, instead of the above two examples simply do:\n",
    "##### This creates \"/Results/bar\" implicitly\n",
    "\n",
    "```python\n",
    "bar = foo.create_group('Results/bar')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset objects are created using the method `Group.create_dataset()`, where `group` os a Group object. \n",
    "#### The `Group.create_dataset()` method takes two mandatory arguments, the positional argument `identifier`, a string used to name the dataset, and the keyword argument `data`, which can be a NumPy `ndarray` or a `list`. For MOHID, we use NumPy `ndarrays` with `float64` for consistency, and because this is what I saw when reverse engineering Shihan's files. \n",
    "#### The `Group.create_dataset()` method also accepts a multitude of other optional keyword arguments, some of which I use:\n",
    "#### - `shape` is a `tuple` that describes the dimensions of `data`\n",
    "#### - `chunks` is a `tuple` that describes the dimensions of the chucnk sizes we want to store `data` in. When revverse engineering Shihan's files, I saw that that `chunks` was the same as `shape` so I left it as is\n",
    "#### - `compression` is a `str`. I use `'gzip'` due to the reasons described __[here](http://docs.h5py.org/en/stable/high/dataset.html#lossless-compression-filters)__, and because 1) it produces an HDF5 file of acceptable size comparable to that of the size produced by Shihan's Matlab scripts 2) it works with MOHID\n",
    "#### - `compression_opts` is an `int` from 1-9. The defualt value is 4.\n",
    "#### You can read more about datasets __[here](http://docs.h5py.org/en/stable/high/dataset.html)__\n",
    "\n",
    "#### Suppose `data` is a NumPy array, defined as follows:\n",
    "```python\n",
    "import numpy\n",
    "dataarray = numpy.ones([100,100]) # a 2D array of shape (100,100) with 1 everywhere\n",
    "dataarray = dataarray.astype('float64') # convert all values to float64 \n",
    "```\n",
    "\n",
    "#### Create a Dataset on the `bar` Group using data from `dataarray`\n",
    "```python\n",
    "bar_00001 = bar.create_dataset(\n",
    "    'bar_00001',\n",
    "    shape = (100,100),\n",
    "    data = dataarray,\n",
    "    chunks = (100,100),\n",
    "    compression = 'gzip',\n",
    "    compression_opts = 1,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata is assigned to a `Dataset` object to give MOHID vital information, such as the FillValue, Maximum and Minimum of a dataset. It is also assigned to a dataset for human convenience, such as the units of the data contained within the dataset\n",
    "\n",
    "#### The `attrs` atribute of a `Dataset` object contains is a `dict` that contains its metadata. It is updated using a python `dict`.\n",
    "\n",
    "#### For instance:\n",
    "```python\n",
    "metadata = {\n",
    "    'FillValue' : np.array([0.]),\n",
    "    'Maximum' : np.array([5.]),\n",
    "    'Minimum' : np.array([-5.]),\n",
    "    'Units' : b'm/s'\n",
    "    }\n",
    "```\n",
    "#### Note: Not all Datasets have a `FillValue` key, such as Time\n",
    "\n",
    "#### `FillValue` is a flag used by MOHID to mask land values. I inherited this from Shihan's matlab scripts. Notice the use of a NumPy array with a float value.\n",
    "#### `Maximum` is a flag used by MOHID to mask land values. I inherited this from Shihan's matlab scripts. Notice the use of a NumPy array with a float value.\n",
    "#### `Minimum` is a flag used by MOHID to mask land values. I inherited this from Shihan's matlab scripts. Notice the use of a NumPy array with a float value.\n",
    "#### `Units` is a flag used by MOHID to mask land values. I inherited this from Shihan's matlab script. It is an `str`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Metadata to a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can use a `dict` to add metadata attributes to a named dataset\n",
    "```python\n",
    "bar_00001.attrs.update(metadata)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To flush the data to disk, the `h5py.File` object must be closed when you are done writing to it\n",
    "```python\n",
    "foo.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
