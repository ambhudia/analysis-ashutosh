{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 File Creation and Conventions Documentation\n",
    "\n",
    "#### Based on my experience with using the __[h5py](http://docs.h5py.org/en/stable/)__ library to create forcing files for MOHID, this notebook documents the recommended way of creating HDF5 files with a tree structure, compression variables and metadata attributes for datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating HDF5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code that follows assumes that the `h5py` python library has been imported:\n",
    "    \n",
    "```python\n",
    "import h5py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an empty HDF5 file. Creating an instance of a  `File` object gives the HDF5 file a `root` or `/` Group and returns a 'File' object which can be assigned to a variable. The code that follows assumes that the `foo` file object is open. \n",
    "\n",
    "```python\n",
    "foo = h5py.File('foo.hdf5', 'w')\n",
    "```\n",
    "#### The first argument that `File` takes is the path and name of the `.hdf5` file that will be created, updated or read. The second argument is the mode with which to access the file. From the __[h5py documentation](http://docs.h5py.org/en/stable/high/file.html)__:\n",
    "| Mode | What it does |\n",
    "| --- | --- |\n",
    "| 'r' | Read only, file must exist |\n",
    "| 'r+' | Read/write, file must exist |\n",
    "| 'w' | Create file, truncate if exists |\n",
    "| 'w-' or x | Create file, fail if exists |\n",
    "| 'a' | Read/write if exists, create otherwise (default) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Tree Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The way MOHID reads in the hdf5 files requires the datasets to be contianed systematically in an arbitrary arity tree (__[from CPSC110](https://eyqs.ca/assets/documents/UBCCPSC110.txt)__: a tree whose nodes have an arbitrary number of children)\n",
    "\n",
    "#### The tree structure is comprised of:\n",
    "#### - Groups: The containers that create the 'nodes'/'branches' of the tree that hold the datasets ('children'/'leaves')\n",
    "#### - Datasets: Homogeneous collections (i.e. all elements are of one type, such as `float64`, `int`, and so on) of data. These are the 'children'/'leaves' of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computer scientists draw their trees upside down\n",
    "#### Consider the example:\n",
    "                            foo.hdf5 (root or / Group)\n",
    "                                |\n",
    "                                |\n",
    "               +------------------------------------+\n",
    "               |                                    |\n",
    "               |                                    |\n",
    "             Time (Group)                        Results (Group)\n",
    "              - Time_00001 (Dataset)                 |\n",
    "              - Time_00002 (Dataset)                 |\n",
    "                                                     |\n",
    "                                         +-----------------------+\n",
    "                                         |                       |\n",
    "                                         |                       |\n",
    "                                      bar (Group)            baz (Group)\n",
    "                                       - bar_00001 (Dataset)  - baz_00001 (Dataset)\n",
    "                                       - bar_00002 (Dataset)  - baz_00002 (Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can have nested Groups, such as `bar` and `baz`, which are children of the `Results` Group\n",
    "#### The tree structure terminates with datasets (You could terminate with another Group, but that would just be an empty container\n",
    "#### All children at a certain level must have unique names, for instance we cannot have two Datasets under `Time` called `Time_00001` or two groups under `Results` called `bar`\n",
    "\n",
    "### Notes: \n",
    "#### 1) ALL the input HDF5 files for MOHID have a `Time` Group and a `Results` Group under which the required Groups are created. This is not configurable without going into the source code. The rest of the Groups can be customised, as I explain in the section 'The HDF5 structures for various input files' below.\n",
    "#### 2) MOHID reads in the data sequentially by referring to the numerical reference. By its convention, all numerical references begin with `'_00001'`. They must be recorded chronologically, for instance, MOHID will crash if the timestamp in `Time_00001` occurs after `Time_00002`\n",
    "#### 3) The names of all Gatasets under a Group must have the same name as its Group, followed by its numerical reference, as seen in the diagram above\n",
    "#### 4) Since Datasets do not contain information about the timestamp they pertain to, you must ensure that the numerical reference ascribed to a Dataset is the same as the numerical reference ascribed to its timestamp e.g. a `bar_01100` will be assigned `Time_01100` by MOHID. You must then be careful only to contain Groups whose Datasets have the same timestamps. For instance, since WaveWatch3 outputs are twice as frequent as SalishSeaCast outputs and are output at different timestamps, they cannot be sored in the same HDF5 forcing file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The HDF5 structures for various input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The names given to the `Groups` are defined in the input file blocks in the `.dat` configuration files\n",
    "\n",
    "#### For instance, the `Hydrodynamic.dat` file we have been using contains the blocks:\n",
    "```dat\n",
    "! Excerpt from Hydrodynamic.dat\n",
    "<begin_waterlevel>\n",
    "NAME                      : water level\n",
    "UNITS                     : m\n",
    "DIMENSION                 : 2D\n",
    "DEFAULTVALUE              : 0\n",
    "INITIALIZATION_METHOD     : hdf\n",
    "FILE_IN_TIME              : hdf\n",
    "FILENAME                  : ./water_levels.hdf5\n",
    "<end_waterlevel>\n",
    "\n",
    "<begin_velocity_u>\n",
    "NAME                      : velocity U\n",
    "UNITS                     : m/s\n",
    "DIMENSION                 : 3D\n",
    "DEFAULTVALUE              : 0\n",
    "INITIALIZATION_METHOD     : hdf\n",
    "FILE_IN_TIME              : hdf\n",
    "FILENAME                  : ./currents.hdf5\n",
    "<end_velocity_u>\n",
    "\n",
    "<begin_velocity_v>\n",
    "NAME                      : velocity V\n",
    "UNITS                     : m/s\n",
    "DIMENSION                 : 3D\n",
    "DEFAULTVALUE              : 0\n",
    "INITIALIZATION_METHOD     : hdf\n",
    "FILE_IN_TIME              : hdf\n",
    "FILENAME                  : ./currents.hdf5\n",
    "<end_velocity_v>\n",
    "\n",
    "<begin_velocity_w>\n",
    "NAME                      : velocity W\n",
    "UNITS                     : m/s\n",
    "DIMENSION                 : 3D\n",
    "DEFAULTVALUE              : 0\n",
    "INITIALIZATION_METHOD     : hdf\n",
    "FILE_IN_TIME              : hdf\n",
    "FILENAME                  : ./currents.hdf5\n",
    "<end_velocity_w>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `NAME` attribute gives the name of the Group that the quantity will be read from under the `Results` group of the input `.hdf5` file.\n",
    "#### The `FILENAME` attribute will be the name of the `.hdf5` file MOHID will look for to find that Group. These will be symlinked to by MOHID-cmd. This means that you must include the filename as recorded in the `.dat` file in the `.yaml` file you use to set off the run under the `forcing` block, for instance:\n",
    "\n",
    "````yaml\n",
    "! .yaml\n",
    "forcing:\n",
    "  currents.hdf5: path of file containing 'velocity U', 'velocity V' and 'velocity W' Groups\n",
    "  water_levels.hdf5: path of file contianing the 'water level' Group\n",
    "````\n",
    "### Note:\n",
    "#### 1) These two file paths can even refer to the same file. As long as the group exists under `Results`, it will be read.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we decide to change the `FILENAME` in the `begin_velocity_w` block to say `./vertical_velocities.hdf5`, we then change the `forcing` block in the `.yaml` file to:\n",
    "\n",
    "```yaml\n",
    "! .yaml\n",
    "forcing:\n",
    "  currents.hdf5: path of file containing 'velocity U' and 'velocity V' Groups\n",
    "  vertical_velocities: path of file containing 'velocity W' Groups\n",
    "  water_levels.hdf5: path of file contianing the 'water level' Group\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This means that we can set the tree structure to be what we want it to be, and can be flexible about what input variables we want to group together. I used the tree strucutres I inherited from Shihan, but you can make your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  For instance, we can have:\n",
    "\n",
    "                         currents.hdf5 (root or / Group)\n",
    "                                |\n",
    "                                |\n",
    "               +------------------------------------+\n",
    "               |                                    |\n",
    "               |                                    |\n",
    "             Time (Group)                        Results (Group)\n",
    "              - Time_00001 (Dataset)                 |\n",
    "              - Time_00002 (Dataset)                 |\n",
    "                                                     |\n",
    "                             +-------------------------------------------------------------------------+\n",
    "                             |                                   |                                     |\n",
    "                             |                                   |                                     |\n",
    "                      Velocity U (Group)                 Velocity V (Group                        Velocity W\n",
    "                       - Velocity U_00001 (Dataset)       - Velocity V_00001 (Dataset)             - Velocity W_00001 (Dataset)\n",
    "                       - Veloctiy U_00002 (Dataset)       - Velocity V_00002 (Dataset)             - Velocity W_00002 (Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or even:\n",
    "\n",
    "                             currents.hdf5 (root or / Group)\n",
    "                                |\n",
    "                                |\n",
    "               +------------------------------------+\n",
    "               |                                    |\n",
    "               |                                    |\n",
    "             Time (Group)                        Results (Group)\n",
    "              - Time_00001 (Dataset)                 |\n",
    "              - Time_00002 (Dataset)                 |\n",
    "                                                     |\n",
    "                                     +-----------------------------------+\n",
    "                                     |                                   |                                     \n",
    "                                     |                                   |                                     \n",
    "                              Velocity U (Group)                 Velocity V (Group              \n",
    "                               - Velocity U_00001 (Dataset)       - Velocity V_00001 (Dataset) \n",
    "                               - Veloctiy U_00002 (Dataset)       - Velocity V_00002 (Dataset) \n",
    "                               \n",
    "#### and\n",
    "\n",
    "                             currents.hdf5 (root or / Group)\n",
    "                                |\n",
    "                                |\n",
    "               +------------------------------------+\n",
    "               |                                    |\n",
    "               |                                    |\n",
    "             Time (Group)                        Results (Group)\n",
    "              - Time_00001 (Dataset)                 |\n",
    "              - Time_00002 (Dataset)                 |\n",
    "                                                     |\n",
    "                                                   +---+\n",
    "                                                     |                                                              \n",
    "                                                     |                                                           \n",
    "                                              Velocity W (Group)                           \n",
    "                                               - Velocity W_00001 (Dataset)     \n",
    "                                               - Veloctiy W_00002 (Dataset)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It depends on how you set it up, and as long as the rules are adhered to, MOHID will accept it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the Examples that follow, I shall be referring to                             \n",
    "\n",
    "                    foo.hdf5 (root or / Group)\n",
    "                                |\n",
    "                                |\n",
    "               +------------------------------------+\n",
    "               |                                    |\n",
    "               |                                    |\n",
    "             Time (Group)                        Results (Group)\n",
    "              - Time_00001 (Dataset)                 |\n",
    "              - Time_00002 (Dataset)                 |\n",
    "                                                     |\n",
    "                                         +-----------------------+\n",
    "                                         |                       |\n",
    "                                         |                       |\n",
    "                                      bar (Group)            baz (Group)\n",
    "                                       - bar_00001 (Dataset)  - baz_00001 (Dataset)\n",
    "                                       - bar_00002 (Dataset)  - baz_00002 (Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group objects are created using the method `Group.create_group()`, where `Group` is a Group object. This method works on a newly created file object like `foo` because the file was created by default with a `root` or `/` Group. The `Group.create_group()` method takes a `name` argument, which is an identifier for the group, and returns a Group object that can be assigned to a variable. You can read more about Groups __[here](http://docs.h5py.org/en/stable/high/group.html#creating-groups)__\n",
    "\n",
    "#### Create a Group on `root` or `/`\n",
    "##### This creates \"/Results\" explicitly\n",
    "\n",
    "```python\n",
    "Results = foo.create_group('Results')\n",
    "```\n",
    "\n",
    "\n",
    "#### Create a nested Group by creating a new child group on an existing group\n",
    "##### This creates \"/Results/bar\" explicitly\n",
    "\n",
    "```python\n",
    "bar = Results.create_group('bar')\n",
    "```\n",
    "\n",
    "\n",
    "#### Implicitly define a Group, for instance, instead of the above two examples simply do:\n",
    "##### This creates \"/Results/bar\" implicitly\n",
    "\n",
    "```python\n",
    "bar = foo.create_group('Results/bar')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset objects are created using the method `Group.create_dataset()`, where `group` os a Group object. \n",
    "#### The `Group.create_dataset()` method takes two mandatory arguments, the positional argument `identifier`, a string used to name the dataset, and the keyword argument `data`, which can be a NumPy `ndarray` or a `list`. For MOHID, we use NumPy `ndarrays` with `float64` for consistency, and because this is what I saw when reverse engineering Shihan's files. \n",
    "#### The `Group.create_dataset()` method also accepts a multitude of other optional keyword arguments, some of which I use:\n",
    "#### - `shape` is a `tuple` that describes the dimensions of `data`\n",
    "#### - `chunks` is a `tuple` that describes the dimensions of the chucnk sizes we want to store `data` in. When revverse engineering Shihan's files, I saw that that `chunks` was the same as `shape` so I left it as is\n",
    "#### - `compression` is a `str`. I use `'gzip'` due to the reasons described __[here](http://docs.h5py.org/en/stable/high/dataset.html#lossless-compression-filters)__, and because 1) it produces an HDF5 file of acceptable size comparable to that of the size produced by Shihan's Matlab scripts 2) it works with MOHID\n",
    "#### - `compression_opts` is an `int` from 1-9. The defualt value is 4.\n",
    "\n",
    "#### You can read more about datasets __[here](http://docs.h5py.org/en/stable/high/dataset.html)__\n",
    "\n",
    "#### Suppose `data` is a NumPy array, defined as follows:\n",
    "```python\n",
    "import numpy\n",
    "data = numpy.ones([100,100]) # a 2D array of shape (100,100) with 1 everywhere\n",
    "data = data.astype('float64') # convert all values to float64 \n",
    "```\n",
    "\n",
    "#### Create a Dataset on the `bar` Group using data from `dataarray`\n",
    "```python\n",
    "bar_00001 = bar.create_dataset(\n",
    "    'bar_00001',\n",
    "    shape = (100,100),\n",
    "    data = data,\n",
    "    chunks = (100,100),\n",
    "    compression = 'gzip',\n",
    "    compression_opts = 1,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata is assigned to a `Dataset` object to give MOHID vital information. The FillValue, Maximum, Minimum and Units of a dataset are the only ones I have encountered so far that MOHID requires when it reads in a Dataset.\n",
    "\n",
    "#### The `attrs` atribute of a `Dataset` object contains is a `dict` that contains its metadata. It is updated using a python `dict`.\n",
    "\n",
    "#### For instance:\n",
    "```python\n",
    "metadata = {\n",
    "    'FillValue' : np.array([0.]),\n",
    "    'Maximum' : np.array([5.]),\n",
    "    'Minimum' : np.array([-5.]),\n",
    "    'Units' : b'm/s'\n",
    "    }\n",
    "```\n",
    "#### Note: Not all Datasets have a `FillValue` key, such as Time\n",
    "\n",
    "#### `FillValue` is a flag used by MOHID to mask land values. I inherited this from Shihan's matlab scripts. Notice the use of a NumPy array with a float value.\n",
    "#### `Maximum` is a flag used by MOHID. I inherited this from Shihan's matlab scripts. Notice the use of a NumPy array with a float value.\n",
    "#### `Minimum` is a flag used by MOHID. I inherited this from Shihan's matlab scripts. Notice the use of a NumPy array with a float value.\n",
    "#### `Units` are assigned to the dataset. I inherited this from Shihan's matlab script. It is an `str`. I suppose that this may be for human convenience but MOHID uses it as well, for instance in `MohidWater/ModuleHydrodynamicFile.F90`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Metadata to a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can use a `dict` to add metadata attributes to a named dataset\n",
    "```python\n",
    "bar_00001.attrs.update(metadata)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To flush the data to disk, the `h5py.File` object must be closed when you are done writing to it\n",
    "```python\n",
    "foo.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
